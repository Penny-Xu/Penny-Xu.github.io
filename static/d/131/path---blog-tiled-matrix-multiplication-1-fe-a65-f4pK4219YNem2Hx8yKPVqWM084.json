{"data":{"markdownRemark":{"frontmatter":{"title":"Tiled matrix multiplication","date":"2019-08-24","tags":["deep learning","matrix multiplication","CUDA"]},"html":"<p>Let's talk about tiled matrix multiplication today. This is an algorithm performed on GPUs due to the parallel nature of matrix multiplication. We will especially look at a method called \"tiling,\" which is used to reduce global memory accesses by taking advantage of the shared memory on the GPU. Tiling can be seen as a way to boost execution efficiency of the kernel. We will then examine the CUDA kernel code that do exactly what we see in the visualization, which shows what each thread within a block is doing to compute the output.</p>\n<p>This post is not meant to teach you CUDA coding, but rather it is meant to help viewers gain some visual intuition on what each thread is doing in a basic tiled matrix multiplication algorithm. I strongly believe that calculating the index and writing the code is not hard if you can see what you are coding.</p>\n<h2>Why should you care?</h2>\n<p>The efficiency of calculating matrix multiplication is the backbone of everything. Everything as in rendering graphics and machine learning. Ever heard of Tensors? Yeah...everything is matrix multiplication I swear.</p>\n<h2>Some background</h2>\n<p>The main idea of using GPUs for computation is simple. The idea is to get more work done in less time. Imagine you have an assignment with 4 math problems to solve, each problem taking 1 hour. You can spend 4 hours and do all 4 problems by yourself. But what if you have 3 other friends with the same assignment? Then you tell your friends to each solve 1 problem and then you all will share the solutions...because sharing is caring. This means in 1 hour, your assignment would be finished.</p>\n<p>To finish off this analogy, each one of your friends is a worker, or an unit of execution, a thread. When you have a lot of workers (threads) to manage, you might want to organize them in a way. Below is the organization of threads in CUDA terms.</p>\n<ul>\n<li><strong>Thread</strong>: single unit of execution --- each thread has its own memory called <em>registers</em></li>\n<li><strong>Block</strong>: group of threads --- all threads in a block has access to a shared memory called <em>shared memory</em></li>\n<li><strong>Grid</strong>: group of blocks --- all threads in a grid has access to <em>global memory</em> and <em>constant memory</em></li>\n</ul>\n<h2>Problem setup</h2>\n<p>Given a 4x4 input matrix A and a 4x4 input matrix B, I want to calculate a 4x4 output matrix C. Since C consists of 16 elements, where each element is computed through a dot product of a row of A and a column of B, then let's launch 16 threads, where each thread calculates 1 output element. For the sake of this example, let's say the threads is organized into a 2x2 block, and there are 4 blocks in a grid.</p>\n<p><img src=\"/setup-2d0f22ecd2e9b7c84af56792d14ba18a.gif\"></p>\n<h2>Visualization</h2>\n<p><img src=\"/tmm-59dd890f48435e692c47919d0df4a5e6.gif\"></p>\n<h2>Kernel code</h2>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"tiled-matrix-multiplication"}}