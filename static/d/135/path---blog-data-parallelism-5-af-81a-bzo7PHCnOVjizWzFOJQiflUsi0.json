{"data":{"markdownRemark":{"frontmatter":{"title":"Parallel Deep Learning","date":"2019-10-20","tags":["neural network","deep learning","parallelism"]},"html":"<p>We all know that GPUs can speed up training time of neural network models over CPUs due to its parallel nature as kinda demonstrated from my previous post. Recently, a friend who read my post asked me...if training on GPUs is so effective, why don't you just have multiple GPUs and stack them up to accelerate training time..because..parallelism. What he was referring is known as multi-processor parallelism. Intuitively, this makes a lot of sense, more hardware resources means more computing power, which means less training time. But now the question is HOW? What is the best way to split up the work load of training a neural network model onto multiple GPUs? Are there any inherent parallelism in the learning workloads? These are the questions that I could not answer...until now! We will investigate the three types of parallelism today:</p>\n<ul>\n<li>Data Parallelism</li>\n<li>Model Parallelism</li>\n<li>Pipelining</li>\n</ul>\n<h2>Initial Thoughts</h2>\n<p>Training neural networks is, surprisingly, a simple problem. Stochastic gradient descent (SDG) with minibatches and its variants remain, at least for now, the most popular methods for training deep neural networks. When you ignore the fancy weight update functions/optimizers (adaptive learning rate, momentum, Adam ect.), SDG essentially becomes a forloop to calculate the gradients and update the weights. The bare bones of SDG for training a neural network literally looks something like this:</p>\n<pre><code>  for number of iterations:\n      for samples in dataset:\n          model.forward\n          loss.backward\n          new weights &#x3C;- old weights - learning_rate * gradient\n</code></pre>\n<p>Below is the above algorithm in a more mathy notation:</p>\n<p><strong>for</strong> <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">t = 0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.61508em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">t</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span></span></span></span> <strong>do</strong><br>\n     <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>s</mi><mo>←</mo></mrow><annotation encoding=\"application/x-tex\">s \\larr</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">←</span></span></span></span> random element from <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span></span></span><br>\n     <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>←</mo><mi>f</mi><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mi>t</mi></msup><mo separator=\"true\">,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f \\larr f(w^t, s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">←</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.043556em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mclose\">)</span></span></span></span><br>\n     <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>←</mo><mi>l</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo separator=\"true\">,</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">l \\larr l(f, h(f))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">←</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">h</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span><br>\n     <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup><mo>←</mo><msup><mi>w</mi><mi>t</mi></msup><mo>−</mo><mi>η</mi><mo>⋅</mo><mi mathvariant=\"normal\">Δ</mi><mi>l</mi><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mi>t</mi></msup><mo separator=\"true\">,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">w^{t+1} \\larr w^t - \\eta \\cdot \\Delta l(w^t, l)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">←</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8768859999999999em;vertical-align:-0.08333em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.63889em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">⋅</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.043556em;vertical-align:-0.25em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose\">)</span></span></span></span><br>\n<strong>end for</strong></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>S</mi></mrow><annotation encoding=\"application/x-tex\">S</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">S</span></span></span></span> is training dataset, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mi>t</mi></msup><mo separator=\"true\">,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(w^t, s)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.043556em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">s</span><span class=\"mclose\">)</span></span></span></span> is model forward function, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>h</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">h(f)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">h</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mclose\">)</span></span></span></span> is the ground truth label, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo separator=\"true\">,</mo><mi>h</mi><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">l(f, h(f))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\">h</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span> is per sample loss function, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">Δ</mi><mi>l</mi><mo stretchy=\"false\">(</mo><msup><mi>w</mi><mi>t</mi></msup><mo separator=\"true\">,</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Delta l(w^t, l)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.043556em;vertical-align:-0.25em;\"></span><span class=\"mord\">Δ</span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7935559999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose\">)</span></span></span></span> is the gradient of loss w.r.t the weights, and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>η</mi></mrow><annotation encoding=\"application/x-tex\">\\eta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.03588em;\">η</span></span></span></span> is the learning rate.</p>\n<p>You have the training dataset that consists of some number of samples. Each sample contributes to the gradient in a small way. We can update the weights after a single sample, or we can compute the average gradients contributed from each sample in a group of samples (minibatch), and then update the weights for each minibatch by simply taking a step in the negative direction. Usually, updating after each minibatch is more stable than updating after each sample because taking the average of the gradient reduces the variance of the gradient estimate. Now, you are probably wondering why can't the minibatch size can be the whole training input, so you can update the weights once for the whole dataset (batch gradient descent). The answer is usually we are limited by memory, aka, the entire dataset will probably not fit in memory if we want to do one pass for the entire dataset. All this talk about SDG and its variants may be confusing, let me clear things up a bit through the summary below:</p>\n<h4>Quick Summary of SDG</h4>\n<h5>Stochastic Gradient Descent - updates weights after EACH sample</h5>\n<ul>\n<li>more frequent updates may result in faster convergence</li>\n<li>more frequent updates is also computationally expensive</li>\n<li>each update is noisy..may be good for getting out of local minima</li>\n<li>noisy = larger variance = non optimal convergence</li>\n</ul>\n<h5>Batch Gradient Descent - updates weights after ALL samples in dataset</h5>\n<ul>\n<li>fewer updates = better computational efficiency</li>\n<li>more stable convergence since each update is a better approximation of true gradient</li>\n<li>Stable can also mean premature convergence :( rip</li>\n<li>no memory to fit whole dataset</li>\n</ul>\n<h5>Minibatch Stochastic Gradient Descent - updates weights after a MINIBATCH of samples</h5>\n<ul>\n<li>best of both worlds of the 2 variants above</li>\n<li>but need to have a hyper parameter of minibatch size</li>\n</ul>\n<p>In the end, minibatch SGD is the most prominently used in deep learning training. So let's assume that the \"samples\" in the above training pseudo code is minibatch. Therefore, we are simply looking at parallelizing the inner forloop by partitionaing the input samples. To be honest, we don't have much to work with. This leads to the first of the three types of parallelism techniques below, called data parallelism. Let's say we have 3 samples and we have 3 GPUs to utilize. The model has some comvolution layers and linear layers shown below. Let's how we can divide up the work into the 3 nodes, <span style=\"color:pink\">GPU1</span>, <span style=\"color:LightBlue\">GPU2</span>, and <span style=\"color:DarkSeaGreen\">GPU1</span>.</p>\n<p><span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 750px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 35.49382716049382%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3oFB/8QAFhABAQEAAAAAAAAAAAAAAAAAIQAQ/9oACAEBAAEFAmc//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAERIVH/2gAIAQEAAT8hnQlisR//2gAMAwEAAgADAAAAEAPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGRABAAMBAQAAAAAAAAAAAAAAAQARIXEx/9oACAEBAAE/EGhw7nIP2Uq8lw2f/9k='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"not\"\n        title=\"\"\n        src=\"/static/3e9f656dc99c7475a4c042e18759ead6/a66cd/not.jpg\"\n        srcset=\"/static/3e9f656dc99c7475a4c042e18759ead6/4272c/not.jpg 188w,\n/static/3e9f656dc99c7475a4c042e18759ead6/1459e/not.jpg 375w,\n/static/3e9f656dc99c7475a4c042e18759ead6/a66cd/not.jpg 750w,\n/static/3e9f656dc99c7475a4c042e18759ead6/c0626/not.jpg 1125w,\n/static/3e9f656dc99c7475a4c042e18759ead6/2d494/not.jpg 1296w\"\n        sizes=\"(max-width: 750px) 100vw, 750px\"\n      />\n  </span></p>\n<h2>Data Parallelism</h2>\n<p>From the training pseudo code above, it is easy to see that a straight forward approach to parallelize the forloop is by processing different groups of minibatch samples on different GPUs. This is similar to the idea of using minibatch gradient descent, but scaled to multiple GPUs. So then it is easy to see that the scaling of data parallelism is naturally defined by the minibatch size. Setting the minibatch size is actually not a trivial problem, as setting it too small will ruin the inherent concurrency in evaluation of the loss function, and setting it too large beyond a certain point will decay the quality of the result. Another approach for data parallelism is ParallelSDG where SDG is ran in parallel, dividing the dataset on multiple GPUs. After convergence of all SDGs, we average the weights of each instance of SDG to obtain the update weight. In the end, the idea of data parallelism is about partitioning the input samples.</p>\n<p><span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 750px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 48.945783132530124%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe9Gklg//8QAFhAAAwAAAAAAAAAAAAAAAAAAAAEg/9oACAEBAAEFAhT/AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABkQAQACAwAAAAAAAAAAAAAAAAEAECEx0f/aAAgBAQABPyFm52mBkr//2gAMAwEAAgADAAAAEMAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAHRABAAEEAwEAAAAAAAAAAAAAAREAEEFxITGRsf/aAAgBAQABPxDrn2KEcH1iwEZCgIgNat//2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"d\"\n        title=\"\"\n        src=\"/static/7036eceb31b3ccbe7cc9342746548634/a66cd/d.jpg\"\n        srcset=\"/static/7036eceb31b3ccbe7cc9342746548634/4272c/d.jpg 188w,\n/static/7036eceb31b3ccbe7cc9342746548634/1459e/d.jpg 375w,\n/static/7036eceb31b3ccbe7cc9342746548634/a66cd/d.jpg 750w,\n/static/7036eceb31b3ccbe7cc9342746548634/c0626/d.jpg 1125w,\n/static/7036eceb31b3ccbe7cc9342746548634/dcdc6/d.jpg 1328w\"\n        sizes=\"(max-width: 750px) 100vw, 750px\"\n      />\n  </span></p>\n<h2>Model Parallelism</h2>\n<p>Data aside, we can also look at the inherent structure of our model to look for parallelism. This strategy divides the work according to the neurons in each layer, basically breaking our model across each GPU, which has the benefit of conserving memory since the full network is not stored in one place. However, this also requires extra synchronization and communication after each layer. Each minibatch is also copied to each GPU.</p>\n<p><span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 750px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 33.38509316770186%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdyAuD//xAAWEAEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQEAAQUCupqP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFhAAAwAAAAAAAAAAAAAAAAAAAAEQ/9oACAEBAAY/AhT/xAAZEAACAwEAAAAAAAAAAAAAAAAAAREhMUH/2gAIAQEAAT8hSKgRLPlmD//aAAwDAQACAAMAAAAQ8A//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAACAgMBAAAAAAAAAAAAAAABEQAhMUFRcf/aAAgBAQABPxAwR278hZoqx7mZ9n//2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"model\"\n        title=\"\"\n        src=\"/static/41301675e21c05473cd94725dd8f86ee/a66cd/model.jpg\"\n        srcset=\"/static/41301675e21c05473cd94725dd8f86ee/4272c/model.jpg 188w,\n/static/41301675e21c05473cd94725dd8f86ee/1459e/model.jpg 375w,\n/static/41301675e21c05473cd94725dd8f86ee/a66cd/model.jpg 750w,\n/static/41301675e21c05473cd94725dd8f86ee/c0626/model.jpg 1125w,\n/static/41301675e21c05473cd94725dd8f86ee/9adec/model.jpg 1288w\"\n        sizes=\"(max-width: 750px) 100vw, 750px\"\n      />\n  </span></p>\n<h2>Pipelining</h2>\n<p>You have probably heard about pipeline relating to computer architecture (5-stage pipeline) where multiple instructions are overlapped during execution. To draw the analogy, each sample in our case is an instruction and each layer is a stage in the pipeline. Basically, each GPU is assigned to and processes a layer. Since each layer depends on the previous layer, we can pipeline it. We can view this method of pipelining as a combination of data parallelism and model parallelism since the samples are processes in parallel as they are fed into the pipeline (data parallelism), while the depth of the pipeline is determined by the number of layers in the model (model parallelism). Below demonstrates the basic idea of pipelining, note the increase in time slices when you don't pipeline.</p>\n<p><span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 750px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 39.42598187311178%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3ZCwf//EABYQAQEBAAAAAAAAAAAAAAAAAAEAEf/aAAgBAQABBQJ2Bi//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAAARD/2gAIAQEABj8CFP/EABkQAAIDAQAAAAAAAAAAAAAAAAABESExcf/aAAgBAQABPyFIUzSfTB//2gAMAwEAAgADAAAAEPPP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGRABAAMBAQAAAAAAAAAAAAAAAQARMSFR/9oACAEBAAE/ELHReQFL0V62W17P/9k='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"pipe\"\n        title=\"\"\n        src=\"/static/451e5619c983241f198a041241b3a71a/a66cd/pipe.jpg\"\n        srcset=\"/static/451e5619c983241f198a041241b3a71a/4272c/pipe.jpg 188w,\n/static/451e5619c983241f198a041241b3a71a/1459e/pipe.jpg 375w,\n/static/451e5619c983241f198a041241b3a71a/a66cd/pipe.jpg 750w,\n/static/451e5619c983241f198a041241b3a71a/c0626/pipe.jpg 1125w,\n/static/451e5619c983241f198a041241b3a71a/c8c19/pipe.jpg 1324w\"\n        sizes=\"(max-width: 750px) 100vw, 750px\"\n      />\n  </span></p>\n<h4>With Pipeline</h4>\n<pre><code>  sample 1:     GPU1  GPU2  GPU3\n  sample 2:           GPU1  GPU2  GPU3\n  sample 3:                 GPU1  GPU2  GPU3\n  time:           1     2     3     4     5\n</code></pre>\n<h4>Without Pipeline</h4>\n<pre><code>  sample 1:     GPU1  GPU2  GPU3\n  sample 2:                       GPU1  GPU2  GPU3\n  sample 3:                                         GPU1  GPU2  GPU3\n  time:           1     2     3     4     5     6     7     8     9\n</code></pre>\n<p>There are several advantages for a multi-processor pipeline over both data and model parallelism. First, parameters can be split up during forward evaluation and back propagation, unlike model parallelism. Second, the input and outputs of each processor is known. Finally, since each GPU always compute the same layers, the weights can remain cached. The main disadvantage is that each stage in the pipeline need to have similar compute rate to ensure that each sample can arrive at the next stage at the same time.</p>\n<h2>Resources</h2>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/1802.09941.pdf\">https://arxiv.org/pdf/1802.09941.pdf</a></li>\n<li><a href=\"https://arxiv.org/pdf/1811.03600.pdf\">https://arxiv.org/pdf/1811.03600.pdf</a></li>\n</ul>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"data-parallelism"}}