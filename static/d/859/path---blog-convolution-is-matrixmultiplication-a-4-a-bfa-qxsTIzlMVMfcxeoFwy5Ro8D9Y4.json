{"data":{"markdownRemark":{"frontmatter":{"title":"Convolution is Matrix Multiplication","date":"2019-11-18","tags":["neural network","deep learning","parallelism","convolution","matrix multiplication"]},"html":"<h2>Initial Thoughts</h2>\n<p>The computation pattern in training a convolutional network is very similar to matrix multiplication: it is both compute intensive and highly parallel.</p>\n<p>convolution == 2d dot product == unrolled 1d dot product == matrix multiplication</p>\n<p>it is pretty fun to think about, that everything we know in life decomposes to matrix multiplication, which we discussed in an earlier post (matrix multiplication is parallel) .. it is just lucky to have an official name. Now thinking about it - convolution is just a 2d dot product - can we all have a vote to call convolution matrix dot product? If we call an operation between two matrices that does a <strong>multiplication followed by summation</strong> through collapsing one dimension <strong>matrix \"multiplication\"</strong> - we should really call an operation between two matrices that does a <strong>dot product followed by summation</strong> through collapsing one dimension <strong>matrix \"dot product\"</strong> - it's only fair!</p>\n<h2>Why do we care?</h2>\n<p>Let's think about convolution in the context of convolution layers in deep learning. Maybe we can use this idea to our advantage when parallelizing the computation of forward pass output in training and inference. The idea is that libraries for linear algebra are already super optimized to do matrix multiplication. So instead of implementing separate kernels to do convolution, we can simply convert convolution operations to a single matrix multiplication. This means that in order to convert convolution to a single matrix multiplication, we would need to unfold and duplicate the inputs to the convolutional kernel so that all elements needed to compute one output element will be stored as one sequential block.</p>\n<h2>Example</h2>\n<ul>\n<li>Input: C x H x W = (3 x 3 x 3)</li>\n<li>Kernel: C x K x K = (3 x 2 x 2)</li>\n<li>Output: H' x W' = (H - K + 1) x (W - K + 1)</li>\n</ul>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"convolution-is-matrixmultiplication"}}