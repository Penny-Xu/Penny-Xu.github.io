{"data":{"markdownRemark":{"frontmatter":{"title":"When to use GPU for training?","date":"2019-09-04","tags":["PyTorch","CUDA"]},"html":"<p>This is going to be a super chill post. While at Peet's Coffee catching up with a friend and enjoying some cafe time, I noticed that his laptop had a nvidia gpu. So naturally, I asked wouldn't it be fun to run some deep learning training models on his laptop and see how it fares with running the same model on my laptop's cpu.</p>\n<h2>The Dataset</h2>\n<p>The training dataset is generated by <code>generate_disc_set</code> where a <code>train_input</code> sample is a random point (x, y) between [-1, 1] and <code>train_label</code> is either 0 or 1, which indicates if a sample input is within a 0.5pi radius circle centered around the origin. 1000 samples are generated to make up the training set.</p>\n<pre><code>  def generate_disc_set(num_points):\n      _input = torch.empty(num_points, 2).uniform_(0, 1)\n      _target = _input.pow(2).sum(1).sub(1 / (2*math.pi)).sign().neg().add(1).div(2).ceil()\n      return _input, _target.long()\n\n  # Generate 1000 training samples and its labels\n  train_input, train_target = generate_disc_set(1000)\n\n  # Normalize the samples\n  train_input = train_input.sub_(train_input.mean()).div_(train_input.std())\n</code></pre>\n<h2>The model</h2>\n<p>I used a simple linear model with Relu activations for this task. I tested the runtime with increasing number of hidden layers, with doubling the number of neurons in each new layer than the previous. So the number of parameters for each model increased exponentially. I simply used <code>nn.sequential</code> to generate the models with varying layers ranging from 2 to 12 layers. Below is an example of an eight layer model I used. With a learning rate of 0.01, these models reaches training errors of 0%, but of course, accuracy is not the main focus of this post.</p>\n<pre><code>  def create_model():\n      return nn.Sequential(\n          nn.Linear(2, 4),\n          nn.ReLU(),\n          nn.Linear(4, 8),\n          nn.ReLU(),\n          nn.Linear(8, 16),\n          nn.ReLU(),\n          nn.Linear(16, 32),\n          nn.ReLU(),\n          nn.Linear(32, 64),\n          nn.ReLU(),\n          nn.Linear(64, 128),\n          nn.ReLU(),\n          nn.Linear(128, 256),\n          nn.ReLU(),\n          nn.Linear(256, 2)\n      )\n\n  model = create_model()\n</code></pre>\n<h2>Training</h2>\n<pre><code>  criterion = nn.CrossEntropyLoss()\n  optimizer = optim.SGD(model.parameters(), lr=0.1)\n  mini_batch_size = 100\n\n  start_time = time.time()\n  for k in range(100):\n\n      acc_loss = 0\n      nb_train_errors = 0\n\n      for n in range(0, train_input.size(0), mini_batch_size):\n          output = model(train_input.narrow(0, n, mini_batch_size))  \n          pred = torch.argmax(output, dim=1)\n          labels = train_target.narrow(0, n, mini_batch_size)\n          nb_train_errors += torch.sum(pred != labels)\n          loss = criterion(output, labels)\n          acc_loss = acc_loss + loss\n          loss.backward()\n\n          # Update parameters         \n          optimizer.step()\n          optimizer.zero_grad()\n\n\n      print('Epoch {:2d}: Train Loss {:6.2f}, Train Error: {:5.02f}%'\n            .format(k,\n                    acc_loss,\n                    (100 * nb_train_errors) / train_input.size(0)))\n\n  time_elasped = time.time() - start_time\n</code></pre>\n<h2>Modifying code for gpu</h2>\n<p>Since I already have the training code in PyTorch that runs on my laptop, my friend will need to run the same training code, but just make sure that the gpu is used for computation. This is easily done when using PyTorch and Nvidia gpu.</p>\n<ul>\n<li>check if the gpu is available with <code>torch.cuda.is_available()</code></li>\n<li>get the gpu name just to make sure with <code>torch.cuda.get_device_name(0)</code></li>\n<li>port model and train dataset tensors to gpu before train loop with <code>.cuda()</code></li>\n</ul>\n<p>In the end, only 3 lines of code needed to change:</p>\n<pre><code>  model = create_model().cuda()\n  train_input = train_input.cuda()\n  train_target = train_target.cuda()\n</code></pre>\n<h2>The result</h2>\n<p>Below is a table of the raw values. Where each column from left to right represent the number of total parameters, the training time cpu (blue line), and the training time on gpu (red line). The total time is calculated as the run time of 100 epochs. This data is graphed below where the horizontal axis is on a log scale for better visual.</p>\n<p><span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 600px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 90.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAASABQDASIAAhEBAxEB/8QAGAABAQADAAAAAAAAAAAAAAAAAAECAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHuJSg1gzB//8QAGRAAAgMBAAAAAAAAAAAAAAAAAAIBMkIz/9oACAEBAAEFAtQLXcC1joolf//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQMBAT8BH//EABQRAQAAAAAAAAAAAAAAAAAAACD/2gAIAQIBAT8BH//EABYQAQEBAAAAAAAAAAAAAAAAAAEQAP/aAAgBAQAGPwLMc1n/xAAaEAADAAMBAAAAAAAAAAAAAAAAATERUaGx/9oACAEBAAE/IUlls9SQmD9kihJif//aAAwDAQACAAMAAAAQoA8A/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPxAf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPxAf/8QAHxAAAgICAQUAAAAAAAAAAAAAAAERIYGxYVFxocHw/9oACAEBAAE/EKulBJz3IuHJeC2QmRwSZDY2M8D9DNyv6y8nV7P/2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"table1\"\n        title=\"\"\n        src=\"/static/70b27367824ce43b6a5fda058076b946/82472/table1.jpg\"\n        srcset=\"/static/70b27367824ce43b6a5fda058076b946/4272c/table1.jpg 188w,\n/static/70b27367824ce43b6a5fda058076b946/1459e/table1.jpg 375w,\n/static/70b27367824ce43b6a5fda058076b946/82472/table1.jpg 600w\"\n        sizes=\"(max-width: 600px) 100vw, 600px\"\n      />\n  </span>\n<span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 750px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 60.836501901140686%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd9USD//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAEFAl//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAACAgMAAAAAAAAAAAAAAAAAARAhMVFx/9oACAEBAAE/IZfS9iwf/9oADAMBAAIAAwAAABDjD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAQACAQUAAAAAAAAAAAAAAAEAETEQIUFRkf/aAAgBAQABPxAxo2uzLC0jyXbLL1DZXkn/2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"chart\"\n        title=\"\"\n        src=\"/static/b716f572acba8c6c148967ba67257f53/a66cd/chart.jpg\"\n        srcset=\"/static/b716f572acba8c6c148967ba67257f53/4272c/chart.jpg 188w,\n/static/b716f572acba8c6c148967ba67257f53/1459e/chart.jpg 375w,\n/static/b716f572acba8c6c148967ba67257f53/a66cd/chart.jpg 750w,\n/static/b716f572acba8c6c148967ba67257f53/0cd83/chart.jpg 1052w\"\n        sizes=\"(max-width: 750px) 100vw, 750px\"\n      />\n  </span></p>\n<p>As you can see, the gpu starts to beat the cpu after 700,000 parameters are used in the model. This result makes sense, since the deeper the model, the more the parameters, and with increased computation per epoch, the more it makes sense to use a gpu for training.</p>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"when-gpu"}}