{"data":{"markdownRemark":{"frontmatter":{"title":"Distributed Deep Learning","date":"2019-10-20","tags":["neural network","deep learning","parallelism"]},"html":"<p><a href=\"https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/\">https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/</a>\n<a href=\"https://arxiv.org/pdf/1811.03600.pdf\">https://arxiv.org/pdf/1811.03600.pdf</a></p>\n<p>We all know that GPUs can speed up training time of neural network models over CPUs due to its parallel nature as kinda demonstrated from my previous post. Recently, a friend who read my post asked me...if training on GPUs is so effective, why don't you just have multiple GPUs and stack them up to decrease training time..because..parallelism. Intuitively, this makes a lot of sense, more hardware resources means more computing power, which means less training time. But now the question is HOW? What is the best way to split up the work load of training a neural network model onto multiple GPUs? Will there be a problem of diminishing returns? These are the questions that I could not answer...until now!</p>\n<h2>Initial Thoughts</h2>\n<p>Training neural networks is, surprisingly, a simple problem. Stochastic gradient descent (SDG) with minibatches and its variants remain, at least for now, the most popular methods for training deep neural networks. When you ignore the fancy optimizers, SDG essentially becomes a for loop to calculate the gradients and update the weights. The bare bones of SDG for training a neural network literally looks something like this:</p>\n<pre><code>  for number of epochs:\n      for samples in dataset:\n          model.forward\n          loss.backward\n          weights_(t) &#x3C;- weights_(t-1) + gradient\n</code></pre>\n<p>You have the training dataset that consists of some number of samples. Each sample contributes to the gradient in a small way. We can update the weights after a single sample, or we can compute the average gradients contributed from each sample in a group of samples (minibatch), and then update the weights for each minibatch by simply taking a step in the negative direction. Usually, updating after each minibatch is more stable than updating after each sample because taking the average of the gradient reduces the variance of the gradient estimate. Now, you are probably wondering why can't the minibatch size can be the whole training input, so you can update the weights once for the whole dataset. The answer is usually we are limited by memory, aka, the entire dataset will probably not fit in memory if we want to do one pass for the entire dataset.</p>\n<p>Let's say that the \"true\" gradient is calculated through the entire dataset. Therefore, the sum of the gradients of each sample from a minibatch is an estimation of the \"true\" gradient. So is there a difference between updating the weights once for the whole dataset vs updating the weights once for each minibatch? Well, the answer is YES and NO.</p>\n<p>YES if the problem is convex:</p>\n<p>NO if the problem is non-convex:</p>\n<p>The above pseudo code is what we are trying to parallelize here. To be honest, we don't have much to work with. This could be good and bad, haha.  </p>\n<ul>\n<li>Data Parallelism</li>\n<li>Model Parallelism</li>\n</ul>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"data-parallelism"}}