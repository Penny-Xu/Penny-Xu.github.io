{"data":{"markdownRemark":{"frontmatter":{"title":"Distributed Deep Learning","date":"2019-10-20","tags":["neural network","deep learning","parallelism"]},"html":"<p><a href=\"https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/\">https://leimao.github.io/blog/Data-Parallelism-vs-Model-Paralelism/</a>\n<a href=\"https://arxiv.org/pdf/1811.03600.pdf\">https://arxiv.org/pdf/1811.03600.pdf</a></p>\n<p>We all know that GPUs can speed up training time of neural network models over CPUs due to its parallel nature as kinda demonstrated from my previous post. Recently, a friend who read my post asked me...if training on GPUs is so effective, why don't you just have multiple GPUs and stack them up to decrease training time..because..parallelism. Intuitively, this makes a lot of sense, more hardware resources means more computing power, which means less training time. But now the question is HOW? What is the best way to split up the work load of training a neural network model onto multiple GPUs? Will there be a problem of diminishing returns? These are the questions that I could not answer...until now!</p>\n<h2>Initial Thoughts</h2>\n<p>Training neural networks is, surprisingly, a simple problem. Stochastic gradient descent (SDG). When you ignore the fancy optimizers, SDG essentially becomes a for loop to calculate the gradients and update the weights. The bare bones of SDG literally looks something like this:</p>\n<pre><code>  for number of epochs:\n      for samples in dataset:\n          model.forward\n          loss.backward\n          new weights &#x3C;- old weights + gradient\n</code></pre>\n<p>You have the training inputs that consists of some number of samples. Each sample contributes to the gradient in a small way. We can update the weights after a single sample, or we can sum up the gradients contributed from each sample in a minibatch, and then update the weights for each minibatch.</p>\n<p>The above pseudo code is what we are trying to parallelize here. To be honest, we don't have much to work with. This could be good and bad, haha.  </p>\n<ul>\n<li>Data Parallelism</li>\n<li>Model Parallelism</li>\n</ul>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"data-parallelism"}}