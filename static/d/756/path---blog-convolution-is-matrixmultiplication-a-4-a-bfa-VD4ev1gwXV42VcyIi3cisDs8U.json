{"data":{"markdownRemark":{"frontmatter":{"title":"Convolution is Matrix Multiplication","date":"2019-11-18","tags":["neural network","deep learning","parallelism","convolution","matrix multiplication"]},"html":"<h2>Initial Thoughts</h2>\n<p>The computation pattern in training a convolutional network is very similar to matrix multiplication: it is both compute intensive and highly parallel.</p>\n<p>convolution == 2d dot product == unrolled 1d dot product == matrix multiplication</p>\n<p>It is pretty fun to think about, that everything we know in life decomposes to matrix multiplication, which we discussed in an earlier post (matrix multiplication is parallel) .. it is just lucky to have an official name. Now thinking about it - convolution is just a 2d dot product - can we all have a vote to call convolution matrix dot product? If we call an operation between two matrices that does a <strong>multiplication followed by summation</strong> through collapsing one dimension <strong>matrix \"multiplication\"</strong> - we should really call an operation between two matrices that does a <strong>dot product followed by summation</strong> through collapsing one dimension <strong>matrix \"dot product\"</strong> - it's only fair!</p>\n<h2>Why do we care?</h2>\n<p>Let's think about convolution in the context of convolution layers in deep learning. Maybe we can use this idea to our advantage when parallelizing the computation of forward pass output in training and inference. The idea is that libraries for linear algebra are already super optimized to do matrix multiplication. So instead of implementing separate kernels to do convolution, we can simply convert convolution operations to a single matrix multiplication. This means that in order to convert convolution to a single matrix multiplication, we would need to unfold and duplicate the inputs to the convolutional kernel so that all elements needed to compute one output element will be stored as one sequential block.</p>\n<h2>Example</h2>\n<p><span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 750px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 52.20125786163522%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdybAD//xAAXEAADAQAAAAAAAAAAAAAAAAABEiAh/9oACAEBAAEFAtYT/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAIP/aAAgBAQAGPwJf/8QAGRABAAIDAAAAAAAAAAAAAAAAAQARIEFR/9oACAEBAAE/IRs5Go2bx//aAAwDAQACAAMAAAAQ88//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAYEAEBAQEBAAAAAAAAAAAAAAABESEAEP/aAAgBAQABPxBgtgzM51QiCk4s3yef/9k='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"conv\"\n        title=\"\"\n        src=\"/static/aa23ccb5f44195070986192224cfdc17/a66cd/conv.jpg\"\n        srcset=\"/static/aa23ccb5f44195070986192224cfdc17/4272c/conv.jpg 188w,\n/static/aa23ccb5f44195070986192224cfdc17/1459e/conv.jpg 375w,\n/static/aa23ccb5f44195070986192224cfdc17/a66cd/conv.jpg 750w,\n/static/aa23ccb5f44195070986192224cfdc17/c0626/conv.jpg 1125w,\n/static/aa23ccb5f44195070986192224cfdc17/44667/conv.jpg 1272w\"\n        sizes=\"(max-width: 750px) 100vw, 750px\"\n      />\n  </span></p>\n<p>We are going to keep this example simple. From above, we have a convolution layer that takes the input as 3 feature maps (input channel C = 3) of size 3 x 3. Let's have the layer reduce the output to 1 feature map (output channel C' = 1) of size 2 x 2.</p>\n<ul>\n<li>Input: C x H x W = (3 x 3 x 3)</li>\n<li>Kernel: C x K x K = (3 x 2 x 2)</li>\n<li>Output: C' H' x W' = (H - K + 1) x (W - K + 1) = (2 x 2)</li>\n</ul>\n<p>Let's see how we can convert this layer to one big matrix multiplication.</p>\n<p>First notice how the both the input and the kernel is 3D. Well in order to use matrix multiplication, we would need to have a 2D input and a 2D kernel so we can treat both like a matrix. This leads to the flattening of the kernel and duplication of the inputs described below.</p>\n<p>First, each channel of the kernel will be linearized and concatenated shown in grey. </p>\n<p>Second, we will rearrange all input elements. Since the results of the convolutions are summed across input features, the input features can be concatenated into one large matrix. The <strong>height</strong> of the unrolled input equals to the number of kernel elements, 12 in our case. The <strong>width</strong> of the unrolled input equals to the number of output elements, which is 4 in our case.</p>\n<p><img src=\"/demo-9af80b0afedfed875b486a194dbd3211.gif\"></p>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"convolution-is-matrixmultiplication"}}