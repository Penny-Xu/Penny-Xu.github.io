---
title: "Do you even know cross entropy?"
date: "06-04-2019"
---

Ok so this is a bit embarrassing. A friend asked me this question when explaining some concepts in this Big Data class I was taking at the time. Although I have learned entropy in my physics, computer engineering, and probability classes, I was unable to explain what it was. Honestly, the thought of entropy, cross entropy, and KL divergence used to scare me...it just sounds so technical. However, engineers just love to make easy things seem difficult!

## Entropy

Ok so we are going to talk about this "entropy" term in respect to $$$. Say there are 3 people (X, Y, and Z). They share a phone messaging plan. Say person X is from generation X and doesn't really use messaging, so he uses around 20% of the plan. Say person Y is from generation Y (millennials), so he will use around 30% of the plan. Now person Z is from generation Z, and of course he uses 50% of the plan.

    distribution y:   X = 0.2
                      Y = 0.3
                      Z = 0.5

Now, every time a person in this plan sends a message, they can be charged a fee of $1, $2, or $3. If I were a capitalist phone company, I would say every message from person X costs $1, every message form person Y costs $2, and every message from person Z costs $3. This assignment would get the most $$$ since person Z sends the most messages so charging him the most will mean a greater sum, and same idea for person Y and X. However, if I were friends with the 3 people and want the total cost to be the least amount, I would assign person X to $3, person Y to $2, and person Z to $1.

So entropy is like being that friend and assigning costs such that in total, the phone plan costs the least amount. So given the distribution of how often each person in a plan uses their phone, I can always find the "entropy". In the entropy equation, log <sup>1</sup>&frasl;<sub>y</sub> can be seen as the encoding cost. In the context of information theory, it means bits, but in our case, it is the "fee" of sending a message, and y represents the phone usage distribution. So y is inversely proportional to log <sup>1</sup>&frasl;<sub>y</sub>.  If y is large, like 50% in our case, then log <sup>1</sup>&frasl;<sub>y</sub> would be small. But if y decreases, like to 30%, then log <sup>1</sup>&frasl;<sub>y</sub> would increase. Basically, the more one uses their phone, the less they are charged.

Entropy can be calculated for any distributions. Let's calculate the entropy for the above distribution y:

H(y) = &Sigma; y log <sup>1</sup>&frasl;<sub>y</sub>

H(y) = 0.2 * log <sup>1</sup>&frasl;<sub>0.2</sub> + 0.3 * log <sup>1</sup>&frasl;<sub>0.3</sub> + 0.5 * log <sup>1</sup>&frasl;<sub>0.5</sub>

H(y) = 0.2 * 2.32 + 0.3 * 1.74 + 0.5 * 1

H(y) = 1.486

## Cross Entropy

For entropy, we effectively used our knowledge of distribution y to assign the "fees" for each person so that the total cost would be the minimum. But what if we don't know the _correct_ distribution? What if we thought that it was actually distribution x? Then cross entropy calculates the total cost using the _wrong_ distribution.

    distribution x:   X = 0.2
                      Y = 0.5
                      Z = 0.3

H(y, x) = &Sigma; y log <sup>1</sup>&frasl;<sub>x</sub>

H(y, x) = 0.2 * log <sup>1</sup>&frasl;<sub>0.2</sub> + 0.3 * log <sup>1</sup>&frasl;<sub>0.5</sub> + 0.5 * log <sup>1</sup>&frasl;<sub>0.3</sub>

H(y, x) = 0.2 * 2.32 + 0.3 * 1 + 0.5 * 1.74

H(y, x) = 1.634


## KL Divergence

Literally just the difference between cross entropy and entropy. Remember, cross entropy is always greater or equal to entropy and it's shown in the example above. Basically KL divergence means how much _extra_ bits or $$$ one would pay if distribution x is used, rather than distribution y.

KL(y, x) = cross entropy - entropy = H(y) - H(y, x)

![](./kb.jpg)
